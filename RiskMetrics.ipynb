{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40572196-dee9-468b-b7d1-ca36478f3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import norm, chi2,gumbel_l\n",
    "import scipy.optimize as sco\n",
    "import datetime\n",
    "from statsmodels.stats.correlation_tools import cov_nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c3be5-6004-4771-a632-a1724782bccf",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a6c706-8517-440c-bf9c-340da8bf068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def halton_sequences(number,base=2):\n",
    "    \n",
    "    #Generate Halton sequences\n",
    "    \n",
    "    inv_base=1/base\n",
    "    \n",
    "    i=number\n",
    "    halton=0\n",
    "    \n",
    "    while i>0:\n",
    "        \n",
    "        digit = i%base\n",
    "        halton=halton + digit*inv_base\n",
    "        i=(i-digit)/base\n",
    "        inv_base=inv_base/base\n",
    "        \n",
    "    return halton\n",
    "\n",
    "def generate_halton(iterations,dimensions=1,base=2):\n",
    "    \n",
    "    #Generate a Halton Sequences at basis k , then shuffles it\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "    matrix=[]\n",
    "    haltons=[]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        halton=halton_sequences(i,base=base)\n",
    "        haltons.append(halton)\n",
    "    \n",
    "    for dim in range(dimensions):\n",
    "        \n",
    "        matrix.append(haltons)\n",
    "    \n",
    "    matrix = rng.permuted(matrix, axis=1)\n",
    "    return matrix\n",
    "\n",
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "def near_psd(x, epsilon=0):\n",
    "    \n",
    "    #Calculate the nearest positive semi definite matrix\n",
    "\n",
    "    if min(np.linalg.eigvals(x))> epsilon:\n",
    "        return x\n",
    "\n",
    "    n = x.shape[0]\n",
    "    var_list = np.array(np.sqrt(np.diag(x)))\n",
    "    y = np.array([[x[i, j]/(var_list[i]*var_list[j]) for i in range(n)] for j in range(n)])\n",
    "\n",
    "    eigval, eigvec = np.linalg.eig(y)\n",
    "    val = np.matrix(np.maximum(eigval, epsilon))\n",
    "    vec = np.matrix(eigvec)\n",
    "    T = 1/(np.multiply(vec, vec) * val.T)\n",
    "    T = np.matrix(np.sqrt(np.diag(np.array(T).reshape((n)) )))\n",
    "    B = T * vec * np.diag(np.array(np.sqrt(val)).reshape((n)))\n",
    "    near_corr = B*B.T    \n",
    "\n",
    "    near_cov = np.array([[near_corr[i, j]*(var_list[i]*var_list[j]) for i in range(n)] for j in range(n)])\n",
    "    return near_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b471ac9-5f6e-4c92-b988-c013e68bc2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def performance(perf,weights):\n",
    "    \n",
    "    #Calculate the performance of a portfolio on a daily basis\n",
    "    \n",
    "    return np.dot(perf,weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6711073-70b7-4bbb-bb8e-0ac8479d4a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rolling_var(returns,weights,window=30,Q=1):\n",
    "\n",
    "    #This function will return the rolling VaR on a x-days window following historical,parametric and multivariate model\n",
    "\n",
    "    value_at_risk=pd.DataFrame()\n",
    "\n",
    "    mean=returns.rolling(window).mean().dropna()\n",
    "    cov=returns.rolling(window).cov().dropna()\n",
    "    corr=returns.rolling(window).corr().dropna()\n",
    "    std=returns.rolling(window).std().dropna()\n",
    "\n",
    "    index=sorted(tuple(set(cov.index.get_level_values(0))))\n",
    "\n",
    "\n",
    "    var={}\n",
    "\n",
    "    for date in index:\n",
    "            \n",
    "            multivariate_var=performance(np.random.multivariate_normal(mean.loc[date],cov.loc[date],10000),weights) \n",
    "            var[date]=np.percentile(multivariate_var,Q)\n",
    "\n",
    "\n",
    "    var=pd.DataFrame(var.values(),index=var.keys())\n",
    "\n",
    "    portfolio=Portfolio(returns).portfolio(weights)\n",
    "\n",
    "    value_at_risk['Historical']=portfolio.rolling(window=window).apply(lambda x:np.percentile(x,Q))\n",
    "    value_at_risk['Parametric']=portfolio.rolling(window=window).std()*norm(loc =0 , scale = 1).ppf(Q/100)\n",
    "    value_at_risk['Multivariate']=var\n",
    "    value_at_risk['Portfolio']=portfolio\n",
    "\n",
    "    return value_at_risk.dropna()\n",
    "\n",
    "def kupiec_test(rolling_var,Q=5):\n",
    "\n",
    "    number_obs=rolling_var.shape[0]\n",
    "    confidence=Q/100\n",
    "\n",
    "    ret=(1+rolling_var['Portfolio']).cumprod()\n",
    "    return_mean=(ret.iloc[-1])**(1/number_obs)-1\n",
    "\n",
    "    stats={}\n",
    "\n",
    "    stats['Proportion of failure']=[]\n",
    "    stats['Kupiec Stat']=[]\n",
    "    stats['P-value']=[]\n",
    "    stats['Model']=[]\n",
    "\n",
    "    for col in rolling_var.columns:\n",
    "\n",
    "        if col=='Portfolio':\n",
    "\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            number_violation=np.sum(np.where(rolling_var[col]>rolling_var['Portfolio'],1,0))\n",
    "            number_non_violation=number_obs-number_violation\n",
    "            proportion_violation=number_violation/number_obs\n",
    "            proportion_non_violation=1-proportion_violation\n",
    "\n",
    "            kupiec=2*np.log((proportion_non_violation/(1-confidence))**(number_non_violation)*\n",
    "                                (proportion_violation/confidence)**number_violation)\n",
    "\n",
    "            p_value=1-chi2.cdf(kupiec,1)\n",
    "\n",
    "        stats['Kupiec Stat'].append(kupiec)\n",
    "        stats['P-value'].append(p_value)\n",
    "        stats['Proportion of failure'].append(proportion_violation)\n",
    "        stats['Model'].append(col)\n",
    "\n",
    "    stats=pd.DataFrame(stats.values(),index=stats.keys(),columns=stats['Model'])\n",
    "    stats=stats.drop(stats.index[3])\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d59b8-eb12-4009-8f54-90940316a6e4",
   "metadata": {},
   "source": [
    "## Portfolio Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd35608-9236-4fb2-b331-f58272b1945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Portfolio:\n",
    "    \n",
    "    #This class allows the user to calculate various metrics of a portfolio\n",
    "    #and also allows to optmize the portfolio with various constraints\n",
    "    \n",
    "    def __init__(self,returns):\n",
    "        \n",
    "        self.returns=returns\n",
    "        \n",
    "    def inventory(self,weights):\n",
    "\n",
    "        dico_ptf=dict(zip(self.returns.columns,weights))\n",
    "\n",
    "        inventory=pd.DataFrame(dico_ptf.values(),index=dico_ptf.keys(),columns=['Weights'])\n",
    "        inventory=inventory.loc[(inventory!=0).any(axis=1)].sort_values(by='Weights',ascending=False)\n",
    "        \n",
    "        return inventory\n",
    "\n",
    "    def portfolio(self,weights):\n",
    "            \n",
    "        portfolio=pd.DataFrame()\n",
    "        portfolio['Portfolio']=np.sum(weights*self.returns,axis=1)\n",
    "        \n",
    "        return portfolio\n",
    "    \n",
    "    def evolution(self,weights):\n",
    "        \n",
    "        portfolio=self.portfolio(weights)\n",
    "        evolution=(1+portfolio).cumprod()*100\n",
    "        \n",
    "        return evolution\n",
    "    \n",
    "    def performance(self,weights):\n",
    "        performance=np.sum(self.returns*weights,axis=1).mean()*252\n",
    "        #performance=(1+np.sum(returns_to_use*weights,axis=1).mean())**252-1\n",
    "        return performance\n",
    "    \n",
    "    def variance(self,weights):\n",
    "        variance=np.sqrt(np.dot(weights.T,np.dot(self.returns.cov(),weights)))*np.sqrt(252)\n",
    "        return variance\n",
    "    \n",
    "    def sharpe_ratio(weights):\n",
    "            return self.performance(weights)/self.variance(weights)\n",
    "\n",
    "    def optimize(self,objective='minimum_variance',constraints=False):\n",
    "        \n",
    "            \n",
    "        def sum_equal_one(weight):\n",
    "            return np.sum(weight) - 1   \n",
    "        \n",
    "        def sharpe_ratio(weights):\n",
    "            return - self.performance(weights)/self.variance(weights)\n",
    "        \n",
    "        def variance(weights):\n",
    "            variance=np.sqrt(np.dot(weights.T,np.dot(self.returns.cov(),weights)))*np.sqrt(252)\n",
    "            return variance\n",
    "        \n",
    "        n_assets = len(self.returns.columns)\n",
    "        weight = np.array([1 / n_assets] * n_assets)\n",
    "        bounds = tuple((0, 1) for _ in range(n_assets))\n",
    "        \n",
    "        if not constraints:\n",
    "            \n",
    "            constraints = [{'type': 'eq', 'fun': sum_equal_one}]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            constraints=[{'type': 'eq', 'fun': sum_equal_one}]+constraints\n",
    "        \n",
    "        if objective=='minimum_variance':\n",
    "\n",
    "            optimum_weights = sco.minimize(variance, weight, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        \n",
    "        elif objective=='sharpe_ratio':\n",
    "            \n",
    "            optimum_weights = sco.minimize(sharpe_ratio, weight, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(\"Objective function undefined\")\n",
    "            \n",
    "            \n",
    "        return optimum_weights.x\n",
    "    \n",
    "    def black_Litterman(self,P,Q,weights,risk_aversion,tau=0.025):\n",
    "        \n",
    "        implied_returns=risk_aversion*self.returns.cov().dot(weights).squeeze()\n",
    "        omega=np.diag(np.diag(P.dot(tau*self.returns.cov()).dot(P.T)))\n",
    "        sigma_scaled=self.returns.cov()*tau\n",
    "        BL_returns= implied_returns + sigma_scaled.dot(P.T).dot(np.linalg.inv(P.dot(sigma_scaled).dot(P.T))+omega).dot(Q-P.dot(implied_returns))\n",
    "        inv_cov=np.linalg.inv(self.returns.cov())\n",
    "        BL_weights=inv_cov.dot(BL_returns)\n",
    "        BL_weights=BL_weights/BL_weights.sum()\n",
    "        \n",
    "        return BL_weights,BL_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34878d2-f683-4e41-8d4d-047bdf77a8fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275423f6-25af-4e58-97ed-6b3ada84fb2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RiskAnalysis(Portfolio):\n",
    "    \n",
    "    \n",
    "    #This class is used to assess various risk of a portfolio such as Market Risk, VaR\n",
    "    # and to know which asset could possibly contribute to it\n",
    "    \n",
    "    def __init__(self,returns):\n",
    "        \n",
    "        self.returns=returns\n",
    "        super().__init__(returns=returns)\n",
    "        \n",
    "\n",
    "    def historical_var(self,weights,last_days=False,Q=5):\n",
    "        \n",
    "        #Return Historical VaR on the Past x days at Q confidence interval\n",
    "\n",
    "        performance=super().portfolio(weights)\n",
    "        \n",
    "        if last_days:\n",
    "            performance=performance[-last_days:]\n",
    "            \n",
    "        var=np.percentile(performance,Q)\n",
    "        cvar=performance[performance<var].mean().values[0]\n",
    "        \n",
    "        return var,cvar\n",
    "    \n",
    "    \n",
    "    def parametric_var(self,weights,Q=0.95,stress_factor=1):\n",
    "        \n",
    "        #Return parametric VaR, where assets follows a Normal Distribution\n",
    "        \n",
    "        intervals=np.arange(Q, 1, 0.0005, dtype=float)\n",
    "        \n",
    "        variance=super().variance(weights)*stress_factor\n",
    "        VaR=variance/np.sqrt(252)*norm(loc =0 , scale = 1).ppf(1-Q)\n",
    "        CVaR=variance/np.sqrt(252)*norm(loc =0 , scale = 1).ppf(1-intervals).mean()\n",
    "        \n",
    "        return VaR,CVaR\n",
    "        \n",
    "        \n",
    "    def multivariate_distribution(self,\n",
    "                    stress_factor=1.0,\n",
    "                    iterations=10000):\n",
    "        \n",
    "        #Return Multivariate Distribution of a portfolio taking into account potential correlation\n",
    "        \n",
    "        num_asset=len(self.returns.columns)\n",
    "        \n",
    "        if type(stress_factor)==float:\n",
    "            \n",
    "            stress_vec=np.linspace(stress_factor,stress_factor,num_asset)\n",
    "            \n",
    "        else:       \n",
    "            stress_vec=stress_factor\n",
    "            \n",
    "        stress_matrix=np.diag(stress_vec)\n",
    "        stress_matrix=pd.DataFrame(stress_matrix,columns=self.returns.columns,index=self.returns.columns)\n",
    "        \n",
    "        stressed_cov=self.returns.cov().dot(stress_matrix)\n",
    "        mean=self.returns.mean()\n",
    "        \n",
    "        multivariate=np.random.multivariate_normal(mean,stressed_cov,iterations)\n",
    "        \n",
    "        return multivariate\n",
    "    \n",
    "    def gaussian_copula(self,iterations=10000,stress_factor=1.0):\n",
    "        \n",
    "       \n",
    "        randoms=np.random.normal(size=(10000,self.returns.shape[1])).T\n",
    "        corr_matrix=self.returns.corr()\n",
    "        \n",
    "        if type(stress_factor)==float:\n",
    "            stress_vec=np.linspace(stress_factor,stress_factor,self.returns.shape[1])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            stress_vec=stress_factor\n",
    "        \n",
    "        if not is_pos_def(corr_matrix):\n",
    "            corr_matrix=cov_nearest(corr_matrix)\n",
    "        \n",
    "        cholesky=np.linalg.cholesky(corr_matrix)\n",
    "        simulation=np.matmul(cholesky,randoms).T\n",
    "        simulation=pd.DataFrame(simulation)\n",
    "        simulation.columns=self.returns.columns\n",
    "\n",
    "        copula_sample=simulation*self.returns.std()*stress_vec+self.returns.mean()\n",
    "        \n",
    "        return copula_sample\n",
    "    \n",
    "    def t_copula(self,iterations=10000,stress_factor=1.0):\n",
    "        \n",
    "\n",
    "        df=self.returns.shape[1]*self.returns.shape[1]//2+self.returns.shape[1]\n",
    "        ChiSquared = np.random.chisquare(df=df, size=iterations)\n",
    "\n",
    "        randoms=np.random.normal(size=(10000,self.returns.shape[1])).T\n",
    "        corr_matrix=self.returns.corr()\n",
    "        \n",
    "        if type(stress_factor)==float:\n",
    "            stress_vec=np.linspace(stress_factor,stress_factor,self.returns.shape[1])\n",
    "            \n",
    "        else:    \n",
    "            stress_vec=stress_factor\n",
    "        \n",
    "        if not is_pos_def(corr_matrix):\n",
    "            \n",
    "            corr_matrix=cov_nearest(corr_matrix)\n",
    "        \n",
    "        cholesky=np.linalg.cholesky(corr_matrix)\n",
    "            \n",
    "\n",
    "        simulation=np.matmul(cholesky,randoms)/np.sqrt(ChiSquared/df)\n",
    "        simulation=pd.DataFrame(simulation.T)\n",
    "        simulation.columns=self.returns.columns\n",
    "\n",
    "        copula_sample=simulation*self.returns.std()*stress_vec+self.returns.mean()\n",
    "        \n",
    "        return copula_sample\n",
    "    \n",
    "    def gumbel_copula(self,iterations=10000,theta=2):\n",
    "        \n",
    "        uniform_sample=np.random.uniform(size=(iterations,self.returns.shape[1]))\n",
    "        gumbel=np.exp(-(-np.log(uniform_sample))**(theta))\n",
    "        scaled_gumbel=norm.ppf(gumbel,loc=self.returns.mean(),scale=self.returns.std())\n",
    "\n",
    "        return scaled_gumbel\n",
    "\n",
    "    def monte_carlo(self,spot,horizon=20/250,iterations=10000,stress_factor=1.0):\n",
    "        \n",
    "        \n",
    "        num_asset=len(self.returns.columns)\n",
    "        #haltons=generate_halton(iterations,num_asset,base=2)\n",
    "        randoms=np.random.normal(size=(10000,num_asset)).T\n",
    "        \n",
    "        # Create a stress matrix to stress the covariance matrix\n",
    "        \n",
    "        if type(stress_factor)==float:\n",
    "            \n",
    "            stress_vec=np.linspace(stress_factor,stress_factor,num_asset)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            stress_vec=stress_factor\n",
    "        \n",
    "        \n",
    "        #Stress the volatilities of the assets\n",
    "        \n",
    "        vol=self.returns.std()*np.sqrt(250)*stress_vec\n",
    "        \n",
    "        #Create a diagonal matrix of the stress factors\n",
    "        \n",
    "        stress_matrix=np.diag(stress_vec)\n",
    "        stress_matrix=pd.DataFrame(stress_matrix,columns=self.returns.columns,index=self.returns.columns)\n",
    "        \n",
    "        #Find nearest PSD matrix and apply cholesky decomposition to create correaltion effect in Monte Carlo\n",
    "        \n",
    "        stressed_cov=self.returns.cov().dot(stress_matrix)\n",
    "        stressed_std=np.sqrt(np.diag(stressed_cov))\n",
    "        corr_matrix=stressed_cov/np.outer(stressed_std,stressed_std)\n",
    "        \n",
    "        if not is_pos_def(corr_matrix):\n",
    "            corr_matrix=cov_nearest(corr_matrix)\n",
    "        \n",
    "        cholesky=np.linalg.cholesky(corr_matrix)\n",
    "            \n",
    "            \n",
    "        drift=np.exp(-0.5*horizon*vol**2)\n",
    "        factors=spot*drift\n",
    "        factors_vec=factors.to_numpy().reshape(num_asset,-1)\n",
    "                \n",
    "        simulation=np.matmul(cholesky,randoms).T\n",
    "        simulation=pd.DataFrame(simulation)\n",
    "        simulation.columns=self.returns.columns\n",
    " \n",
    "        \n",
    "        monte_carlo=factors_vec.T*np.exp(simulation.dot(np.diag(vol))*np.sqrt(horizon))\n",
    "        monte_carlo=pd.DataFrame(monte_carlo)\n",
    "        monte_carlo.columns=self.returns.columns\n",
    "        perf_monte_carlo=np.log(monte_carlo/spot)\n",
    "        \n",
    "        return monte_carlo,perf_monte_carlo\n",
    "      \n",
    "\n",
    "    \n",
    "    def pca(self,num_components=2):\n",
    "        \n",
    "        #Returns the eigen vectors of the covariance matrix\n",
    "        \n",
    "        cov_matrix=self.returns.cov()\n",
    "                \n",
    "        eig_val, eig_vec=np.linalg.eig(cov_matrix)\n",
    "        sorted_eig_val=eig_val.argsort()[::-1]\n",
    "        eig_val=eig_val[sorted_eig_val]\n",
    "        eig_vec=eig_vec[:,sorted_eig_val]\n",
    "        eig_val=eig_val[:num_components]\n",
    "        eig_vec=eig_vec[:,0:num_components]\n",
    "        \n",
    "        PC={}\n",
    "        \n",
    "        for i in range(eig_vec.shape[1]):\n",
    "            \n",
    "            PC[\"PC\" +str(i+1)]=eig_vec[:,i]/eig_vec[:,i].sum()\n",
    "        \n",
    "        \n",
    "        portfolio_components=pd.DataFrame(PC.values(),index=PC.keys(),columns=self.returns.columns).T\n",
    "        \n",
    "        return eig_val,eig_vec,portfolio_components\n",
    "    \n",
    "\n",
    "    def var_contrib(self,weights):\n",
    "        \n",
    "        weights_matrix=np.diag(weights)\n",
    "        variance_contrib=np.dot(weights_matrix,np.dot(self.returns.cov(),weights_matrix.T))\n",
    "        \n",
    "        asset_contrib=variance_contrib.sum(axis=0)    \n",
    "        diag=np.diag(variance_contrib.diagonal())\n",
    "        variance_decomposition=np.column_stack([asset_contrib,variance_contrib.diagonal(),(variance_contrib-diag).sum(axis=0)])\n",
    "        contrib=pd.DataFrame(variance_decomposition,index=self.returns.columns,columns=['Variance Contribution','Idiosyncratic Risk','Correlation'])\n",
    "        \n",
    "        weighted_covar=pd.DataFrame(variance_contrib,columns=self.returns.columns,index=self.returns.columns)\n",
    "        \n",
    "        return contrib,weighted_covar\n",
    "    \n",
    "    def var_contrib_pct(self,weights):\n",
    "        \n",
    "        var_contrib=self.var_contrib(weights)[0]\n",
    "        var_contrib=var_contrib/var_contrib['Variance Contribution'].sum()\n",
    "        var_contrib.columns=['Variance Contribution in %','Idiosyncratic Risk in %','Correlation in %']\n",
    "        var_contrib=var_contrib.loc[(var_contrib!=0).any(axis=1)]\n",
    "        var_contrib=var_contrib.sort_values('Variance Contribution in %', ascending=False)\n",
    "    \n",
    "        return var_contrib\n",
    "    \n",
    "    def perf_contrib(self,start_weights):\n",
    "    \n",
    "        ptf_evolution=((1+self.returns).cumprod()*start_weights)\n",
    "        initial_weights=self.inventory(start_weights)\n",
    "        \n",
    "        last_nav=ptf_evolution.iloc[-1].sum()\n",
    "        last_row=ptf_evolution.iloc[-1]\n",
    "        last_weight=last_row/last_nav\n",
    "\n",
    "        perf=last_row/start_weights\n",
    "        perf_contrib=(perf-1)*start_weights\n",
    "\n",
    "        perf_report=pd.concat([perf_contrib,initial_weights,last_weight],axis=1)\n",
    "        perf_report.columns=['Performance Contribution','Initial Weights','Last Weights']\n",
    "\n",
    "        return perf_report.dropna()\n",
    "    \n",
    "    def perf_contrib_pct(self,weights):\n",
    "        \n",
    "        perf_contrib=self.perf_contrib(weights)\n",
    "        perf_contrib['Performance Contribution']=perf_contrib['Performance Contribution']/perf_contrib['Performance Contribution'].sum()\n",
    "        perf_contrib.columns=['Performance Contribution in %','Initial Weights','Last Weights']\n",
    "        \n",
    "        perf_report=perf_contrib.sort_values(by='Performance Contribution in %',ascending=False)\n",
    "        \n",
    "        return perf_contrib\n",
    "    \n",
    "    def summary(self,weights):\n",
    "        \n",
    "        inventory=self.inventory(weights)\n",
    "        perf_report=self.perf_contrib_pct(weights)\n",
    "        var_contrib_pct=self.var_contrib_pct(weights)\n",
    "\n",
    "        report=pd.concat([inventory,perf_report,var_contrib_pct],axis=1)\n",
    "        \n",
    "        return report.dropna()\n",
    "    \n",
    "    def tracking_error(self,ptf,bench):\n",
    "        \n",
    "        excess_return=ptf-bench\n",
    "        tracking_error=excess_return.std()*np.sqrt(252)\n",
    "        \n",
    "        return tracking_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2cc30-674c-4c81-bf75-00f5cb766cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f23a7b-abc1-492b-a2f0-19a8e9c15851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
