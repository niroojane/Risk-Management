{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40572196-dee9-468b-b7d1-ca36478f3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import norm, chi2,gumbel_r\n",
    "import scipy.optimize as sco\n",
    "import datetime\n",
    "from statsmodels.stats.correlation_tools import cov_nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c3be5-6004-4771-a632-a1724782bccf",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a6c706-8517-440c-bf9c-340da8bf068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def halton_sequences(number,base=2):\n",
    "    \n",
    "    #Generate Halton sequences\n",
    "    \n",
    "    inv_base=1/base\n",
    "    \n",
    "    i=number\n",
    "    halton=0\n",
    "    \n",
    "    while i>0:\n",
    "        \n",
    "        digit = i%base\n",
    "        halton=halton + digit*inv_base\n",
    "        i=(i-digit)/base\n",
    "        inv_base=inv_base/base\n",
    "        \n",
    "    return halton\n",
    "\n",
    "def generate_halton(iterations,dimensions=1,base=2):\n",
    "    \n",
    "    #Generate a Halton Sequences at basis k , then shuffles it\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "    matrix=[]\n",
    "    haltons=[]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        halton=halton_sequences(i,base=base)\n",
    "        haltons.append(halton)\n",
    "    \n",
    "    for dim in range(dimensions):\n",
    "        \n",
    "        matrix.append(haltons)\n",
    "    \n",
    "    matrix = rng.permuted(matrix, axis=1)\n",
    "    return matrix\n",
    "\n",
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "def near_psd(x, epsilon=0):\n",
    "    \n",
    "    #Calculate the nearest positive semi definite matrix\n",
    "\n",
    "    if min(np.linalg.eigvals(x))> epsilon:\n",
    "        return x\n",
    "\n",
    "    n = x.shape[0]\n",
    "    var_list = np.array(np.sqrt(np.diag(x)))\n",
    "    y = np.array([[x[i, j]/(var_list[i]*var_list[j]) for i in range(n)] for j in range(n)])\n",
    "\n",
    "    eigval, eigvec = np.linalg.eig(y)\n",
    "    val = np.matrix(np.maximum(eigval, epsilon))\n",
    "    vec = np.matrix(eigvec)\n",
    "    T = 1/(np.multiply(vec, vec) * val.T)\n",
    "    T = np.matrix(np.sqrt(np.diag(np.array(T).reshape((n)) )))\n",
    "    B = T * vec * np.diag(np.array(np.sqrt(val)).reshape((n)))\n",
    "    near_corr = B*B.T    \n",
    "\n",
    "    near_cov = np.array([[near_corr[i, j]*(var_list[i]*var_list[j]) for i in range(n)] for j in range(n)])\n",
    "    return near_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b471ac9-5f6e-4c92-b988-c013e68bc2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def performance(perf,weights):\n",
    "    \n",
    "    #Calculate the performance of a portfolio on a daily basis\n",
    "    \n",
    "    return np.dot(perf,weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6711073-70b7-4bbb-bb8e-0ac8479d4a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rolling_var(returns,weights,window=30,Q=1):\n",
    "\n",
    "    #This function will return the rolling VaR on a x-days window following historical,parametric and multivariate model\n",
    "\n",
    "    value_at_risk=pd.DataFrame()\n",
    "\n",
    "    mean=returns.rolling(window).mean().dropna()\n",
    "    cov=returns.rolling(window).cov().dropna()\n",
    "    corr=returns.rolling(window).corr().dropna()\n",
    "    std=returns.rolling(window).std().dropna()\n",
    "\n",
    "    index=sorted(tuple(set(cov.index.get_level_values(0))))\n",
    "\n",
    "\n",
    "    var={}\n",
    "\n",
    "    for date in index:\n",
    "            \n",
    "            multivariate_var=performance(np.random.multivariate_normal(mean.loc[date],cov.loc[date],10000),weights) \n",
    "            var[date]=np.percentile(multivariate_var,Q)\n",
    "\n",
    "\n",
    "    var=pd.DataFrame(var.values(),index=var.keys())\n",
    "\n",
    "    portfolio=Portfolio(returns).portfolio(weights)\n",
    "\n",
    "    value_at_risk['Historical']=portfolio.rolling(window=window).apply(lambda x:np.percentile(x,Q))\n",
    "    value_at_risk['Parametric']=portfolio.rolling(window=window).std()*norm(loc =0 , scale = 1).ppf(Q/100)\n",
    "    value_at_risk['Multivariate']=var\n",
    "    value_at_risk['Portfolio']=portfolio\n",
    "\n",
    "    return value_at_risk.dropna()\n",
    "\n",
    "def kupiec_test(rolling_var,Q=5):\n",
    "\n",
    "    number_obs=rolling_var.shape[0]\n",
    "    confidence=Q/100\n",
    "\n",
    "    ret=(1+rolling_var['Portfolio']).cumprod()\n",
    "    return_mean=(ret.iloc[-1])**(1/number_obs)-1\n",
    "\n",
    "    stats={}\n",
    "\n",
    "    stats['Proportion of failure']=[]\n",
    "    stats['Kupiec Stat']=[]\n",
    "    stats['P-value']=[]\n",
    "    stats['Model']=[]\n",
    "\n",
    "    for col in rolling_var.columns:\n",
    "\n",
    "        if col=='Portfolio':\n",
    "\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            number_violation=np.sum(np.where(rolling_var[col]>rolling_var['Portfolio'],1,0))\n",
    "            number_non_violation=number_obs-number_violation\n",
    "            proportion_violation=number_violation/number_obs\n",
    "            proportion_non_violation=1-proportion_violation\n",
    "\n",
    "            kupiec=2*np.log((proportion_non_violation/(1-confidence))**(number_non_violation)*\n",
    "                                (proportion_violation/confidence)**number_violation)\n",
    "\n",
    "            p_value=1-chi2.cdf(kupiec,1)\n",
    "\n",
    "        stats['Kupiec Stat'].append(kupiec)\n",
    "        stats['P-value'].append(p_value)\n",
    "        stats['Proportion of failure'].append(proportion_violation)\n",
    "        stats['Model'].append(col)\n",
    "\n",
    "    stats=pd.DataFrame(stats.values(),index=stats.keys(),columns=stats['Model'])\n",
    "    stats=stats.drop(stats.index[3])\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d59b8-eb12-4009-8f54-90940316a6e4",
   "metadata": {},
   "source": [
    "## Portfolio Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd35608-9236-4fb2-b331-f58272b1945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Portfolio:\n",
    "    \n",
    "    #This class allows the user to calculate various metrics of a portfolio\n",
    "    #and also allows to optmize the portfolio with various constraints\n",
    "    \n",
    "    def __init__(self,returns):\n",
    "        \n",
    "        self.returns=returns\n",
    "        \n",
    "    def inventory(self,weights):\n",
    "\n",
    "        dico_ptf=dict(zip(self.returns.columns,weights))\n",
    "\n",
    "        inventory=pd.DataFrame(dico_ptf.values(),index=dico_ptf.keys(),columns=['Weights'])\n",
    "        inventory=inventory.loc[(inventory!=0).any(axis=1)].sort_values(by='Weights',ascending=False)\n",
    "        \n",
    "        return inventory\n",
    "\n",
    "    def portfolio(self,weights):\n",
    "            \n",
    "        portfolio=pd.DataFrame()\n",
    "        portfolio['Portfolio']=np.sum(weights*self.returns,axis=1)\n",
    "        \n",
    "        return portfolio\n",
    "    \n",
    "    def evolution(self,weights):\n",
    "        \n",
    "        portfolio=self.portfolio(weights)\n",
    "        evolution=(1+portfolio).cumprod()*100\n",
    "        \n",
    "        return evolution\n",
    "    \n",
    "    def performance(self,weights):\n",
    "        performance=np.sum(self.returns*weights).mean()*252\n",
    "        return performance\n",
    "    \n",
    "    def variance(self,weights):\n",
    "        variance=np.sqrt(np.dot(weights.T,np.dot(self.returns.cov(),weights)))*np.sqrt(252)\n",
    "        return variance\n",
    "    \n",
    "    def sharpe_ratio(weights):\n",
    "            return self.performance(weights)/self.variance(weights)\n",
    "\n",
    "    def optimize(self,objective='minimum_variance',constraints=False):\n",
    "        \n",
    "            \n",
    "        def sum_equal_one(weight):\n",
    "            return np.sum(weight) - 1   \n",
    "        \n",
    "        def sharpe_ratio(weights):\n",
    "            return - self.performance(weights)/self.variance(weights)\n",
    "        \n",
    "        def variance(weights):\n",
    "            variance=np.sqrt(np.dot(weights.T,np.dot(self.returns.cov(),weights)))*np.sqrt(252)\n",
    "            return variance\n",
    "        \n",
    "        n_assets = len(self.returns.columns)\n",
    "        weight = np.array([1 / n_assets] * n_assets)\n",
    "        bounds = tuple((0, 1) for _ in range(n_assets))\n",
    "        \n",
    "        if not constraints:\n",
    "            \n",
    "            constraints = [{'type': 'eq', 'fun': sum_equal_one}]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            constraints=[{'type': 'eq', 'fun': sum_equal_one}]+constraints\n",
    "        \n",
    "        if objective=='minimum_variance':\n",
    "\n",
    "            optimum_weights = sco.minimize(variance, weight, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        \n",
    "        elif objective=='sharpe_ratio':\n",
    "            \n",
    "            optimum_weights = sco.minimize(sharpe_ratio, weight, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(\"Objective function undefined\")\n",
    "            \n",
    "            \n",
    "        return optimum_weights.x\n",
    "    \n",
    "    def black_Litterman(self,P,Q,weights,risk_aversion,tau=0.025):\n",
    "        \n",
    "        implied_returns=risk_aversion*self.returns.cov().dot(weights).squeeze()\n",
    "        omega=np.diag(np.diag(P.dot(tau*self.returns.cov()).dot(P.T)))\n",
    "        sigma_scaled=self.returns.cov()*tau\n",
    "        BL_returns= implied_returns + sigma_scaled.dot(P.T).dot(np.linalg.inv(P.dot(sigma_scaled).dot(P.T))+omega).dot(Q-P.dot(implied_returns))\n",
    "        inv_cov=np.linalg.inv(self.returns.cov())\n",
    "        BL_weights=inv_cov.dot(BL_returns)\n",
    "        BL_weights=BL_weights/BL_weights.sum()\n",
    "        \n",
    "        return BL_weights,BL_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34878d2-f683-4e41-8d4d-047bdf77a8fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2cc30-674c-4c81-bf75-00f5cb766cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f23a7b-abc1-492b-a2f0-19a8e9c15851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
